{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    Text Normalizer class normalizes text\n",
    "    \"\"\"\n",
    "    def __init__(self ):\n",
    "        self.contractions={\n",
    "                            \"ain't\": \"is not\",\n",
    "                            \"aren't\": \"are not\",\n",
    "                            \"can't\": \"cannot\",\n",
    "                            \"can't've\": \"cannot have\",\n",
    "                            \"'cause\": \"because\",\n",
    "                            \"could've\": \"could have\",\n",
    "                            \"couldn't\": \"could not\",\n",
    "                            \"couldn't've\": \"could not have\",\n",
    "                            \"didn't\": \"did not\",\n",
    "                            \"doesn't\": \"does not\",\n",
    "                            \"don't\": \"do not\",\n",
    "                            \"hadn't\": \"had not\",\n",
    "                            \"hadn't've\": \"had not have\",\n",
    "                            \"hasn't\": \"has not\",\n",
    "                            \"haven't\": \"have not\",\n",
    "                            \"he'd\": \"he would\",\n",
    "                            \"he'd've\": \"he would have\",\n",
    "                            \"he'll\": \"he will\",\n",
    "                            \"he'll've\": \"he he will have\",\n",
    "                            \"he's\": \"he is\",\n",
    "                            \"how'd\": \"how did\",\n",
    "                            \"how'd'y\": \"how do you\",\n",
    "                            \"how'll\": \"how will\",\n",
    "                            \"how's\": \"how is\",\n",
    "                            \"I'd\": \"I would\",\n",
    "                            \"I'd've\": \"I would have\",\n",
    "                            \"I'll\": \"I will\",\n",
    "                            \"I'll've\": \"I will have\",\n",
    "                            \"I'm\": \"I am\",\n",
    "                            \"I've\": \"I have\",\n",
    "                            \"i'd\": \"i would\",\n",
    "                            \"i'd've\": \"i would have\",\n",
    "                            \"i'll\": \"i will\",\n",
    "                            \"i'll've\": \"i will have\",\n",
    "                            \"i'm\": \"i am\",\n",
    "                            \"i've\": \"i have\",\n",
    "                            \"isn't\": \"is not\",\n",
    "                            \"it'd\": \"it would\",\n",
    "                            \"it'd've\": \"it would have\",\n",
    "                            \"it'll\": \"it will\",\n",
    "                            \"it'll've\": \"it will have\",\n",
    "                            \"it's\": \"it is\",\n",
    "                            \"let's\": \"let us\",\n",
    "                            \"ma'am\": \"madam\",\n",
    "                            \"mayn't\": \"may not\",\n",
    "                            \"might've\": \"might have\",\n",
    "                            \"mightn't\": \"might not\",\n",
    "                            \"mightn't've\": \"might not have\",\n",
    "                            \"must've\": \"must have\",\n",
    "                            \"mustn't\": \"must not\",\n",
    "                            \"mustn't've\": \"must not have\",\n",
    "                            \"needn't\": \"need not\",\n",
    "                            \"needn't've\": \"need not have\",\n",
    "                            \"o'clock\": \"of the clock\",\n",
    "                            \"oughtn't\": \"ought not\",\n",
    "                            \"oughtn't've\": \"ought not have\",\n",
    "                            \"shan't\": \"shall not\",\n",
    "                            \"sha'n't\": \"shall not\",\n",
    "                            \"shan't've\": \"shall not have\",\n",
    "                            \"she'd\": \"she would\",\n",
    "                            \"she'd've\": \"she would have\",\n",
    "                            \"she'll\": \"she will\",\n",
    "                            \"she'll've\": \"she will have\",\n",
    "                            \"she's\": \"she is\",\n",
    "                            \"should've\": \"should have\",\n",
    "                            \"shouldn't\": \"should not\",\n",
    "                            \"shouldn't've\": \"should not have\",\n",
    "                            \"so've\": \"so have\",\n",
    "                            \"so's\": \"so as\",\n",
    "                            \"that'd\": \"that would\",\n",
    "                            \"that'd've\": \"that would have\",\n",
    "                            \"that's\": \"that is\",\n",
    "                            \"there'd\": \"there would\",\n",
    "                            \"there'd've\": \"there would have\",\n",
    "                            \"there's\": \"there is\",\n",
    "                            \"they'd\": \"they would\",\n",
    "                            \"they'd've\": \"they would have\",\n",
    "                            \"they'll\": \"they will\",\n",
    "                            \"they'll've\": \"they will have\",\n",
    "                            \"they're\": \"they are\",\n",
    "                            \"they've\": \"they have\",\n",
    "                            \"to've\": \"to have\",\n",
    "                            \"wasn't\": \"was not\",\n",
    "                            \"we'd\": \"we would\",\n",
    "                            \"we'd've\": \"we would have\",\n",
    "                            \"we'll\": \"we will\",\n",
    "                            \"we'll've\": \"we will have\",\n",
    "                            \"we're\": \"we are\",\n",
    "                            \"we've\": \"we have\",\n",
    "                            \"weren't\": \"were not\",\n",
    "                            \"what'll\": \"what will\",\n",
    "                            \"what'll've\": \"what will have\",\n",
    "                            \"what're\": \"what are\",\n",
    "                            \"what's\": \"what is\",\n",
    "                            \"what've\": \"what have\",\n",
    "                            \"when's\": \"when is\",\n",
    "                            \"when've\": \"when have\",\n",
    "                            \"where'd\": \"where did\",\n",
    "                            \"where's\": \"where is\",\n",
    "                            \"where've\": \"where have\",\n",
    "                            \"who'll\": \"who will\",\n",
    "                            \"who'll've\": \"who will have\",\n",
    "                            \"who's\": \"who is\",\n",
    "                            \"who've\": \"who have\",\n",
    "                            \"why's\": \"why is\",\n",
    "                            \"why've\": \"why have\",\n",
    "                            \"will've\": \"will have\",\n",
    "                            \"won't\": \"will not\",\n",
    "                            \"won't've\": \"will not have\",\n",
    "                            \"would've\": \"would have\",\n",
    "                            \"wouldn't\": \"would not\",\n",
    "                            \"wouldn't've\": \"would not have\",\n",
    "                            \"y'all\": \"you all\",\n",
    "                            \"y'all'd\": \"you all would\",\n",
    "                            \"y'all'd've\": \"you all would have\",\n",
    "                            \"y'all're\": \"you all are\",\n",
    "                            \"y'all've\": \"you all have\",\n",
    "                            \"you'd\": \"you would\",\n",
    "                            \"you'd've\": \"you would have\",\n",
    "                            \"you'll\": \"you will\",\n",
    "                            \"you'll've\": \"you will have\",\n",
    "                            \"you're\": \"you are\",\n",
    "                            \"you've\": \"you have\"\n",
    "                        }\n",
    "        self.stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "    def normalize(self, text, clean=True, rm_stopwords=True, rm_special_chars=True, \n",
    "                  expand_conts=True, caseConvert=False, lemmatize=True, stem=True):\n",
    "        if clean:\n",
    "            text=self.cleanText(text)\n",
    "        if expand_conts:\n",
    "            text=self.expandContractions(text)\n",
    "        if lemmatize:\n",
    "            text=self.lemmatizeText(text)\n",
    "        if stem:\n",
    "            text=self.stemText(text)\n",
    "        if rm_stopwords:\n",
    "            text=self.removeStopwords(text)\n",
    "        if rm_special_chars:\n",
    "            text=self.removeSpecialChars(text)\n",
    "        if caseConvert:\n",
    "            text=self.caseConvert(text, True)\n",
    "        else:\n",
    "            text=self.caseConvert(text, False)\n",
    "        return text\n",
    "        \n",
    "    def caseConvert(self, text, upper=True):\n",
    "        if upper:\n",
    "            return text.upper()\n",
    "        return text.lower()\n",
    "    \n",
    "    def cleanText(self, page):\n",
    "        soup = BeautifulSoup(page)\n",
    "        fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "        return fetched_text\n",
    "    \n",
    "    def tokenizeText(self,text):\n",
    "        words= nltk.word_tokenize(text)\n",
    "        tokens= [word.strip() for word in words]\n",
    "        return tokens \n",
    "    \n",
    "    def removeStopwords(self, text):\n",
    "        tokens= self.tokenizeText(text)\n",
    "        filter_tokens=[token for token in tokens if token not in self.stopwords]\n",
    "        text=' '.join(filter_tokens)\n",
    "        return text\n",
    "    \n",
    "    def removeSpecialChars(self,text):\n",
    "        tokens=self.tokenizeText(text)\n",
    "        pattern= re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "        filtered_tokens= filter(None, [pattern.sub('',token) for token in tokens])\n",
    "        filtered_text= ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "    \n",
    "    def expandContractions(self, text):\n",
    "        expanded_sentence=\"\"\n",
    "        words=text.split()\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in self.contractions.keys():\n",
    "                words[i]=self.contractions[words[i]]\n",
    "        for word in words:\n",
    "            if word!=words[len(words)-1]:\n",
    "                expanded_sentence+=word+' '\n",
    "            else:\n",
    "                expanded_sentence+=word\n",
    "        return expanded_sentence\n",
    "    \n",
    "    def lemmatizeText(self, text):\n",
    "        return text\n",
    "    \n",
    "    def stemText(self, text):\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TextNormalizer in module __main__:\n",
      "\n",
      "class TextNormalizer(builtins.object)\n",
      " |  Text Normalizer class normalizes text\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  caseConvert(self, text, upper=True)\n",
      " |  \n",
      " |  cleanText(self, page)\n",
      " |  \n",
      " |  expandContractions(self, text)\n",
      " |  \n",
      " |  lemmatizeText(self, text)\n",
      " |  \n",
      " |  normalize(self, text, clean=True, rm_stopwords=True, rm_special_chars=True, expand_conts=True, caseConvert=False, lemmatize=True, stem=True)\n",
      " |  \n",
      " |  removeSpecialChars(self, text)\n",
      " |  \n",
      " |  removeStopwords(self, text)\n",
      " |  \n",
      " |  stemText(self, text)\n",
      " |  \n",
      " |  tokenizeText(self, text)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my name is ali abbas'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn= TextNormalizer()\n",
    "help(TextNormalizer)\n",
    "tn.normalize(\"<h1><p>My name is Ali Abbas</p></h1>\", clean=True, caseConvert=False, rm_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
